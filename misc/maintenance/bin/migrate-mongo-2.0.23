#!/usr/bin/env oo-ruby
require 'rubygems'
$:.unshift('/var/www/openshift/broker')
require 'config/environment'

$users_failed = []

def get_mongo_session
  # Get Mongo session
  config = Mongoid::Config.sessions["default"]
  session = Moped::Session.new(config["hosts"])
  session.use config["database"]
  session.login(config["username"], config["password"])
  session
end

def release_mongo_session(session)
  # Exit session
  session.logout
end

def create_cloud_user(user)
  cu = CloudUser.new
  cu.login = user["login"] 
  cu.consumed_gears = user["consumed_gears"]
  cu.capabilities["max_gears"] = user["max_gears"]
  if user["capabilities"]
    user_cap = user["capabilities"]
    cu.capabilities["subaccounts"] = user_cap["subaccounts"] if user_cap["subaccounts"]
    cu.capabilities["gear_sizes"] = user_cap["gear_sizes"] if user_cap["gear_sizes"]
    cu.capabilities["max_storage_per_gear"] = user_cap["max_storage_per_gear"] if user_cap["max_storage_per_gear"]
  end
  user["ssh_keys"].each do |ssh_key_name, ssh_key|
    sk = SshKey.new
    sk.name = ssh_key_name
    sk.type = ssh_key["type"]
    sk.content = ssh_key["key"]
    cu.ssh_keys << sk
  end if user["ssh_keys"]
  begin
    cu.parent_user_id = CloudUser.find_by(login: user["parent_user_login"])._id if user["parent_user_login"]
  rescue Exception => e
    raise Exception.new "Unable to find parent user #{user["parent_user_login"]}"
  end
  cu.plan_id = user["plan_id"]  if user["plan_id"]
  cu.pending_plan_id = user["pending_plan_id"]  if user["pending_plan_id"]
  cu.pending_plan_uptime = user["pending_plan_uptime"]  if user["pending_plan_uptime"]
  cu.usage_account_id = user["usage_account_id"] if user["usage_account_id"]
  cu.save!
  Lock.create_lock(cu)
  cu
end

def create_domains(user, cloud_user_obj)
  dm = nil
  if user["domains"]
    num_doms = user["domains"].length
    return nil if num_doms==0
    dom_index = 0
    if num_doms > 1
      domain_map = {}
      user['apps'].each { |a|
        domain_map[a['namespace']] = a
      }
      if domain_map.length > 1
        raise Exception.new "Found more than 1 domain in use" 
      end
      dom_index = user["domains"].index { |dom| dom["namespace"]==domain_map.keys[0] }  if domain_map.length==1
    end
    domain = user["domains"][dom_index]
    dm = Domain.new
    dm.namespace = domain["namespace"]
    dm.canonical_namespace = dm.namespace.downcase
    dm.owner_id = cloud_user_obj._id
    dm.user_ids << cloud_user_obj._id 
    user["system_ssh_keys"].each do |ssh_key_name, ssh_key|
      ssk = SystemSshKey.new
      ssk.name = ssh_key_name
      ssk.type = "ssh-rsa"
      ssk.content = ssh_key
      dm.system_ssh_keys << ssk
    end if user["system_ssh_keys"]
    #dm.env_vars =
    dm.save!
  end
  dm
end

def create_applications(user, domain_obj)
  user["apps"].each do |uapp|
    app = Application.new(domain: domain_obj)
    #app._id = Moped::BSON::ObjectId.from_string(uapp["uuid"].to_s)
    app.uuid = uapp["uuid"]
    app.name = uapp["name"]
    app.created_at = Time.parse(uapp["creation_time"])
    app.canonical_name = app.name.downcase
    app.scalable = uapp["scalable"]
    app.init_git_url = uapp["init_git_url"] if uapp["init_git_url"]
    app.default_gear_size = uapp["node_profile"]
    uapp["ssh_keys"].each do |ssh_key_name, ssh_key|
      ask = ApplicationSshKey.new
      ask.name = ssh_key_name
      ask.type = "ssh-rsa"
      ask.content = ssh_key
      app.app_ssh_keys << ask
    end if uapp["ssh_keys"]
    app.domain_id = domain_obj._id
    cinst_map = {}
    uapp["comp_instances"].each do |comp_inst|
      next if comp_inst["parent_cart_name"] == app.name
      if cinst_map[comp_inst["parent_cart_name"]]
        ci = cinst_map[comp_inst["parent_cart_name"]]
        ci.component_properties = ci.component_properties.merge(comp_inst["cart_properties"])
      else
        ci = ComponentInstance.new
        ci.cartridge_name = comp_inst["parent_cart_name"]
        ci.component_name = CartridgeCache.find_cartridge(ci.cartridge_name).profiles[0].components[0].name
        ci.component_properties = comp_inst["cart_properties"] if comp_inst["cart_properties"]
        app.component_instances << ci
        cinst_map[comp_inst["parent_cart_name"]] = ci
      end
    end if uapp["comp_instances"]
    app.aliases = []
    uapp["aliases"].each do |al|
      app.aliases << al.downcase
    end if uapp["aliases"]
    group_overrides = create_group_instances(app, uapp)
    #  app.group_overrides = uapp["group_overrides"] if uapp["group_overrides"]
    fix_connections(app, group_overrides)
    app.save!
  end if user["apps"]
end

def fix_connections(app, group_overrides)
  features = app.requires
  group_overrides = group_overrides + (app.scalable ? [] : (app.gen_non_scalable_app_overrides(features)))
  connections, new_groups, new_group_overrides = app.elaborate(features, group_overrides)
  app.set_connections(connections)
  app.group_overrides = new_group_overrides
end

def create_group_instances(new_app, old_app)
  new_comp_inst_hash = {}
  group_override_map = {}
  new_app.component_instances.each { |ci|
    new_comp_inst_hash[ci.cartridge_name] = ci
  }

  old_cinst_hash = {}
  old_app['comp_instances'].each { |old_ci|
    old_cinst_hash[old_ci['name']] = old_ci
  }
  old_app['group_instances'].each { |old_gi|
    this_gis_carts = []
    is_singleton = (not new_app.scalable)
    is_app_dns = (not new_app.scalable)
    framework_cart = nil
    old_gi['component_instances'].each { |comp_name|
      cartname = old_cinst_hash[comp_name]['parent_cart_name']
      next if cartname == new_app.name
      is_app_dns = true if cartname == 'haproxy-1.4' unless is_app_dns
      cart = CartridgeCache.find_cartridge(cartname)
      is_singleton = cart.get_component(new_comp_inst_hash[cartname].component_name).is_singleton? unless is_singleton
      if framework_cart.nil?
        web_framework_carts = CartridgeCache.cartridge_names("web_framework")
        framework_cart = { "comp" => new_comp_inst_hash[cartname].component_name, "cart" => cartname } if web_framework_carts.include? cartname
      end
      this_gis_carts << cartname unless this_gis_carts.include? cartname
    }
    new_gi_id = nil
    addtl_fs_gb = nil
    this_gis_carts.each { |cartname|
      new_gi_id = new_comp_inst_hash[cartname].group_instance_id
      break unless new_gi_id.nil?
    }
    if not new_gi_id
      # create a new group instance
      new_gi = GroupInstance.new(custom_id: Moped::BSON::ObjectId.new, gear_size: old_gi['node_profile'])
      new_app.group_instances.push(new_gi)
      addtl_fs_gb = old_gi['addtl_fs_gb']
      new_gi_id = new_gi._id
    else
      # gears from old_gi need to be put to new_gi_id
      new_gi = new_app.group_instances.find(new_gi_id)
      if addtl_fs_gb.nil? and old_gi['addtl_fs_gb']
        addtl_fs_gb = old_gi['addtl_fs_gb']
      elsif old_gi['addtl_fs_gb'] and addtl_fs_gb < old_gi['addtl_fs_gb']
        # raise warning
        puts "WARNING : Migration of app '#{new_app.name}'/'#{new_app.uuid}' will have some gears with mismatched additional filesystem storage"
        addtl_fs_gb = old_gi['addtl_fs_gb']
      end
    end
    count = 0
    old_gi['gears'].each { |old_gear|
      count += 1
      if count == 1 
        singleton = is_singleton 
        app_dns = is_app_dns
      else 
        singleton = false
        app_dns = false
      end
      if app_dns
        gear_id = new_app._id
      else
        gear_id = Moped::BSON::ObjectId.new
      end
      gear = Gear.new(custom_id: gear_id, group_instance: new_gi, host_singletons: singleton, app_dns: app_dns,
                      uuid: old_gear['uuid'], uid: old_gear['uid'], server_identity: old_gear['server_identity'])
      new_gi.gears.push(gear)
    }
    # finally fix the component instances of the new app to adopt the new group instance and its gears
    this_gis_carts.each { |cartname|
      if new_comp_inst_hash[cartname].group_instance_id.nil? 
        new_comp_inst_hash[cartname].group_instance_id = new_gi_id 
      elsif new_comp_inst_hash[cartname].group_instance_id != new_gi_id
        raise Exception.new "Group instance migration failed. app: #{new_app.name}, cart: #{cartname}, expected group_instance_id: #{new_gi_id}, found group_instance_id: #{new_comp_inst_hash[cartname].group_instance_id}"
      end
    }

    # calculate group overrides based on scale min/max
    group_override = group_override_map[new_gi_id.to_s]
    gi_comps = new_comp_inst_hash.values.select { |ci| ci.group_instance_id==new_gi_id }
    group_components = gi_comps.map { |ci| { "comp" => ci.component_name, "cart" => ci.cartridge_name } }
    group_override["components"] = group_components if group_override

    if old_gi['supported_min']!=old_gi['supported_max'] and framework_cart
      group_override = { "components" => group_components } if group_override.nil?
      if old_gi['max']!=-1
        max_gears = old_gi['max']+1
        group_override['max_gears'] = max_gears
      end
      min_gears = old_gi['min']+1
      group_override['min_gears'] = min_gears
      group_override_map[new_gi_id.to_s] = group_override
    end

    # group override for addtl_fs_gb
    if addtl_fs_gb and addtl_fs_gb > 0
      group_override = { "components" => group_components } if group_override.nil?
      group_override["additional_filesystem_gb"] = addtl_fs_gb
      group_override_map[new_gi_id.to_s] = group_override
    end
  }
  group_override_map.values.dup
end

def migrate_user(user)
  begin
    cu = create_cloud_user(user)
    dm = create_domains(user, cu)
    create_applications(user, dm)
  rescue Exception => e
    puts "ERROR: Migration failed for User #{user['login']}: #{e.message}"
    puts e.backtrace
    $users_failed << user['login']
    return 1
  end
end

def migrate_district(old_district)
  nd = District.new(name: old_district['name'], gear_size: old_district['node_profile'])
  old_district.delete('node_profile')
  old_district.delete('creation_time')
  nd.assign_attributes(old_district)
  nd.available_capacity = nd.available_uids.length
  nd.save
  nd
end

puts "Migration Started..."
session = get_mongo_session
begin

  # cleanup if starting from scratch
  session[:cloud_users].find.remove_all
  session[:applications].find.remove_all
  session[:districts].find.remove_all
  session[:locks].find.remove_all
  session[:domains].find.remove_all

  users = session[:user]

  # Process all users that DOES NOT have a parent user login
  users.find({"$or"=>[{"parent_user_login"=>{"$exists"=>false}}, {"parent_user_login" => nil}]}).each do |user|
    puts "Processing user #{user['login']}"
    #puts user.inspect
    migrate_user(user)
  end
  # Process all user that have a parent user login
  users.find({"$and"=>[{"parent_user_login"=>{"$exists"=>true}}, {"parent_user_login"=>{"$ne"=>nil}}]}).each do |user|
    puts "Processing user #{user['login']}"
    #puts user.inspect
    migrate_user(user)
  end

  puts "User migration complete. Migrating districts..."
  session[:district].find.each { |old_district|
    nd = migrate_district(old_district)
    if nd.uuid==old_district['uuid']
      puts "Successfully migrated district #{nd.name} (#{nd.uuid})"
    else
      puts "Failed to completely migrate district #{old_district['uuid']}"
      $users_failed << "District uuid #{old_district['uuid']}"
    end
  }

rescue Exception => e
  puts e.message
  puts e.backtrace
  exit 1
ensure
  release_mongo_session(session)
end
puts "***************************"
if $users_failed.empty?
  puts "Migration Done! - No failures"
else
  puts "Migration Done! - #{$users_failed.length} failures"
  puts "Failed user logins:"
  puts $users_failed.join("\n")
end
puts "***************************"
